
# Про Big O notation

Ну чисто я каждый раз, когда вспоминаю про существование данного вопроса:

![[Pasted image 20240611214405.png]]

## **Время и память — основные характеристики алгоритмов**

[Алгоритмы](https://skillbox.ru/media/code/polzay-kak-muravey-letay-kak-pchela/?utm_source=media&utm_medium=link&utm_campaign=all_all_media_links_links_articles_all_all_skillbox) описывают с помощью двух характеристик — _времени_ и _памяти_.

**Время** — это… время, которое нужно алгоритму для обработки данных. Например, для маленького массива — 10 секунд, а для массива побольше — 100 секунд. Интуитивно понятно, что время выполнения алгоритма зависит от размера массива.

Но есть проблема: секунды, минуты и часы — это не очень показательные единицы измерения. Кроме того, время работы алгоритма зависит от железа, на котором он выполняется, и других внешних факторов. Поэтому время считают не в секундах и часах, а в количестве операций, которые алгоритм совершит. Это надёжная и, главное, независимая от железа метрика.

Когда говорят о _Time Complexity_ или просто _Time_, то речь идёт именно о количестве операций. Для простоты расчётов разница в скорости между операциями обычно опускается. Поэтому, несмотря на то, что деление чисел с плавающей точкой требует от процессора больше действий, чем сложение целых чисел, обе операции в теории алгоритмов считаются равными по сложности.

> [!NOTE]
> Запомните: в О-нотации на операции с одной или двумя переменными вроде _i++_, _a * b_, _a / 1024_, _max(a,b)_ уходит всего одна единица времени.

**Память, или место,** — это объём оперативной памяти, который потребуется алгоритму для работы. Одна переменная — это одна ячейка памяти, а массив с тысячей ячеек — тысяча ячеек памяти.

В теории алгоритмов все ячейки считаются равноценными. Например, int a на 4 байта и double b на 8 байт имеют один вес. Потребление памяти обычно называется _Space Complexity_ или просто _Space_, редко — _Memory_.

Алгоритмы, которые используют исходный массив как рабочее пространство, называют _in-place_. Они потребляют мало памяти и создают одиночные переменные — без копий исходного массива и промежуточных структур данных. Алгоритмы, требующие дополнительной памяти, называют _out-of-place_. Прежде чем использовать алгоритм, надо понять, хватит ли на него памяти, и если нет — поискать менее прожорливые альтернативы.


## **Что такое O(n)**

Давайте вспомним 8-й класс и разберёмся: что значит запись _O(n)_ с математической точки зрения. При расчёте Big O Notation используют два правила:

**Константы откидываются.** Нас интересует только часть формулы, которая зависит от размера входных данных. Проще говоря, это само число n, его степени, [логарифмы, факториалы и экспоненты](https://skillbox.ru/media/code/matematika-v-zhizni/?utm_source=media&utm_medium=link&utm_campaign=all_all_media_links_links_articles_all_all_skillbox), где число находится в степени n.

**Примеры:**

_O(3n) = O(n)_

_O(10000 n^2) = O(n^2)_

_O(2n * log n) = O(n * log n)_

**Если в O есть сумма, нас интересует самое быстрорастущее слагаемое.** Это называется асимптотической оценкой сложности.

**Примеры:**

_O(n^2 + n) = O(n^2)_

_O(n^3 + 100n * log n) = O(n^3)_

_O(n! + 999) = O(n!)_

_O(1,1^n + n^100) = O(1,1^n)_

## **Хороший, плохой, средний**

У каждого алгоритма есть худший, средний и лучший _сценарии_ работы — в зависимости от того, насколько удачно выбраны входные данные. Часто их называют _случаями_.

**Худший случай (worst case)** — это когда входные данные требуют максимальных затрат времени и памяти.

Например, если мы хотим отсортировать массив по возрастанию (Ascending Order, коротко ASC), то для простых алгоритмов сортировки худшим случаем будет массив, отсортированный по убыванию (Descending Order, коротко DESC).

Для алгоритма поиска элемента в неотсортированном массиве worst case — это когда искомый элемент находится в конце массива или если элемента нет вообще.

**Лучший случай** **(best case)** — полная противоположность worst case, самые удачные входные данные. Правильно отсортированный массив, с которым алгоритму сортировки вообще ничего делать не нужно. В случае поиска — когда алгоритм находит нужный элемент с первого раза.

**Средний случай** **(average case)** — самый хитрый из тройки. Интуитивно понятно, что он сидит между best case и worst case, но далеко не всегда понятно, где именно. Часто он совпадает с worst case и всегда хуже best case, если best case не совпадает с worst case. Да, иногда они совпадают.

Как определяют средний случай? Считают статистически усреднённый результат: берут алгоритм, прокручивают его с разными данными, составляют сводку результатов и смотрят, вокруг какой функции распределились результаты. В общем, расчёт average case — дело сложное. А мы приступаем к конкретным алгоритмам.

## **Линейный поиск**

Начнём с самого простого алгоритма — _линейного поиска_, он же _linear search_. Дальнейшее объяснение подразумевает, что вы знаете, что такое числа и как устроены массивы. Напомню, это всего лишь набор проиндексированных ячеек.

Допустим, у нас есть массив целых чисел _arr_, содержащий n элементов. Вообще, количество элементов, размер строк, массивов, списков и графов в алгоритмах всегда обозначают буквой _n_ или _N_. Ещё дано целое число _x_. Для удобства обусловимся, что _arr_ точно содержит _x_.

**Задача:** найти, на каком месте в массиве _arr_ находится элемент _3_, и вернуть его индекс.

![](https://skillbox.ru/upload/setka_images/07031317012022_accf102caaa970ce65d217b9ae9a8e9a57caa67c.jpg)

Фото: Валерий Жила для Skillbox Media

Меткий человеческий глаз сразу видит, что искомый элемент содержится в ячейке с индексом 2, то есть в _arr[2]_. А менее зоркий компьютер будет перебирать ячейки друг за другом: _arr[0], arr[1]_… и так далее, пока не встретит тройку или конец массива, если тройки в нём нет.

Теперь разберём случаи:

**Worst case.** Больше всего шагов потребуется, если искомое число стоит в конце массива. В этом случае придётся перебрать все n ячеек, прочитать их содержимое и сравнить с искомым числом. Получается, worst case равен _O(n)_. В нашем массиве худшему случаю соответствует _x = 2_.

**Best case.** Если бы искомое число стояло в самом начале массива, то мы бы получили ответ уже в первой ячейке. Best case линейного поиска — _O(1)_. Именно так обозначается константное время в Big O Notation. В нашем массиве best case наблюдается при _x = 7_.

**Average case.** В среднем случае результаты будут равномерно распределены по массиву. Средний элемент можно рассчитать по формуле (n + 1) / 2, но так как мы отбрасываем константы, то получаем просто _n._ Значит, average case равен _O(n)_. Хотя иногда в среднем случае константы оставляют, потому что запись O(n / 2) даёт чуть больше информации.

## **Как определить сложность алгоритма**

Мы рассмотрели два алгоритма и увидели примеры их сложности. Но так и не поговорили о том, как эту сложность определять. Есть три основных способа.

### **Оценка «на глаз»**

Первый и наиболее часто используемый способ. Именно так мы определяли сложность linear search и binary search. Обобщим эти примеры.

**Первый случай.** Есть алгоритм _some_function_, который выполняет действие _А_, а после него — действие _В_. На _А_ и _В_ нужно _K_ и _J_ операций соответственно.

`   int some_function(): 	action_A() // K operations 	action_B() // J operations   `

В случае последовательного выполнения действий сложность алгоритма будет равна _O(K + J)_, а значит, _O(max (K, J))_. Например, если _А_ равно _n^2_, а _В_ — _n_, то сложность алгоритма будет равна _O(n^2 + n)_. Но мы уже знаем, что нас интересует только самая быстрорастущая часть. Значит, ответ будет _O(n^2)._

**Второй случай.** Посчитаем сложность действий или вызова методов в циклах. Размер массива равен _n_, а над каждым элементом будет выполнено действие _А (_n раз_)._ А дальше всё зависит от «содержимого» _A_.

Посчитаем сложность бинарного поиска:

`   int some_function(int [] arr): 	n = arr.length 	for i in 0..n: 	    action_A() // K operations   `

Если на каждом шаге _A_ работает с одним элементом, то, независимо от количества операций, получим сложность _O(n)_. Если же _A_ обрабатывает _arr_ целиком, то алгоритм совершит n операций n раз. Тогда получим _O(n *_ _n) = O(n^2)_. По такой же логике можно получить O(n * log n), O(n^3) и так далее.

**Третий случай — комбо.** Для закрепления соединим оба случая. Допустим, действие _А_ требует _log(n)_ операций, а действие _В_ — _n_ операций. На всякий случай напомню: в алгоритмах всегда идёт речь о двоичных логарифмах.

Добавим действие _С_ с пятью операциями и вот что получим:

`   int some_function(int [] arr): 	n = arr.length 	for i in 0..n: 	    for j in 0..n: 	        action_A(arr) // log(n) operations 	    action_B(arr) // n operations    action_C() // 5 operations   `

O(n * (n * log(n) + n) + 5) = O(n^2 * log(n) + n^2 + 5) = O(n^2 * log(n)).

Мы видим, что самая дорогая часть алгоритма — действие _А_, которое выполняется во вложенном цикле. Поэтому именно оно доминирует в функции.

Есть разновидность определения на глаз — амортизационный анализ. Это относительно редкий, но достойный упоминания гость. В двух словах его можно объяснить так: если на _X_ «дешёвых» операций (например, с O(1)) приходится одна «дорогая» (например, с O(n)), то на большом количестве операций суммарная сложность получится неотличимой от _O(1)_.

Частый пациент амортизационного анализа — _динамический массив_. Это массив, который при переполнении создаёт новый, больше оригинального в два раза. При этом элементы старого массива копируются в новый.

Практически всегда добавление элементов в такой массив «дёшево» — требует лишь одной операции. Но когда он заполняется, приходится тратить силы: создавать новый массив и копировать _N_ старых элементов в новый. Но так как массив каждый раз увеличивается в два раза, переполнения случаются всё реже и реже, поэтому average case добавления элемента равен _O(1)_.

### **Мастер-теорема**

Слабое место прикидывания на глаз — рекурсия. С ней и правда приходится тяжко. Поэтому для оценки сложности рекурсивных алгоритмов широко используют [мастер-теорему](https://ru.wikipedia.org/wiki/%D0%9E%D1%81%D0%BD%D0%BE%D0%B2%D0%BD%D0%B0%D1%8F_%D1%82%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%BE_%D1%80%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D1%85_%D1%81%D0%BE%D0%BE%D1%82%D0%BD%D0%BE%D1%88%D0%B5%D0%BD%D0%B8%D1%8F%D1%85).

По сути, это набор правил по оценке сложности. Он учитывает, сколько новых ветвей рекурсии создаётся на каждом шаге и на сколько частей дробятся данные в каждом шаге рекурсии. Это если вкратце.

### **Метод Монте-Карло**

[Метод Монте-Карло](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%9C%D0%BE%D0%BD%D1%82%D0%B5-%D0%9A%D0%B0%D1%80%D0%BB%D0%BE) применяют довольно редко — только если первые два применить невозможно. Особенно часто с его помощью описывают производительность систем, состоящих из множества алгоритмов.

Суть метода: берём алгоритм и гоняем его на случайных данных разного размера, замеряем время и память. Полученные измерения выкладываем на отдельные графики для памяти и времени. А затем автоматически вычисляется функция, которая лучше всего описывает полученное облако точек.

## **Как ещё оценивают сложность алгоритмов**

На протяжении всей статьи мы говорили про Big O Notation. А теперь сюрприз: это только одна из пяти существующих нотаций. Вот они слева направо: Намджун, Чонгук, Чингачгук… простите, не удержался. Сверху вниз: _Small o_, _Big O_, _Big Theta_, _Big Omega_, _Small omega_. _f_ — это реальная функция сложности нашего алгоритма, а _g_ — асимптотическая.

![](https://skillbox.ru/upload/setka_images/07072017012022_fc9956ee2f4201e204a5532c68850c6715ed24e0.jpg)

Пять нотаций в математическом представлении. Фото: Валерий Жила для Skillbox Media

Несколько слов об этой весёлой компании:

- **Big O** обозначает верхнюю границу сложности алгоритма. Это идеальный инструмент для поиска worst case.
- **Big Omega** (которая пишется как подкова) обозначает нижнюю границу сложности, и её правильнее использовать для поиска best case.
- **Big Theta** (пишется как О с чёрточкой) располагается между О и омегой и показывает точную функцию сложности алгоритма. С её помощью правильнее искать average case.
- **Small o** и **Small omega** находятся по краям этой иерархии и используются в основном для сравнения алгоритмов между собой.

«Правильнее» в данном контексте означает — с точки зрения математических пейперов по алгоритмам. А в статьях и рабочей документации, как правило, во всех случаях используют «Большое „О“».

Если хотите подробнее узнать об остальных нотациях, посмотрите [интересный видос](https://www.youtube.com/watch?v=1tfdr1Iv6JA) на эту тему. Также полезно понимать, как сильно отличаются скорости возрастания различных функций. Вот хороший [cheat sheet](https://www.bigocheatsheet.com/) по сложности алгоритмов и наглядная картинка с графиками оттуда:

![](https://skillbox.ru/upload/setka_images/07074817012022_6caf85fa09e0642959e62c753d9a2f18236eb1da.jpg)

[Сравнение](https://www.bigocheatsheet.com/) сложности алгоритмов. Скриншот: Валерий Жила для Skillbox Media

Хоть картинка и наглядная, она плохо передаёт всю бездну, лежащую между функциями. Поэтому я склепал таблицу со значениями времени для разных функций и N. За время одной операции взял 1 наносекунду:

![](https://skillbox.ru/upload/setka_images/07080217012022_d0e289e355555cb39f9d7f499b6888c389473c54.png)

Источник: Валерий Жила. Таблица: Евгений Рыбкин / Skillbox Media

## **Скрытые константы**

В последнем разделе поговорим о скрытых константах. Это хитрая штука, там собака зарыта.

Возьмём умножение матриц. При размерности матрицы n * n наивный алгоритм, который многие знают с начальных курсов универа, «строчка на столбик» имеет кубическую временную сложность _O(n^3)_. Кубическая, зато честная. Без констант и _O(10000_ _*_ _n^3)_ под капотом. И памяти не ест — только время.

У Валерия есть отдельный [тред](https://twitter.com/ValeriiZhyla/status/1447493685323739136) об умножении матриц.

В некоторых алгоритмах умножения матриц степень _n_ сильно «порезана». Самые «быстрые» из них выдают время около _O(n^2,37)_. Какой персик, правда? Почему бы нам не забыть про «наивный» алгоритм?

Проблема в том, что у таких алгоритмов огромные константы. В пейперах гонятся за более компактной экспонентой, а степени отбрасываются. Я не нашёл внятных значений констант. Даже авторы оригинальных пейперов называют их просто «очень большими».

Давайте от балды возьмём не очень большую константу 100 и сравним _n^3_ c _100 * n^2,37_. Правая функция даёт выигрыш по сравнению с левой для n, начинающихся с 1495. А ведь мы взяли довольно скромную константу. Подозреваю, что на практике они не в пример больше…

В то же время умножение матриц 1495 × 1495 — очень редкий случай. А матрицы миллиард на миллиард точно нигде не встретишь. Да простит меня Император Душнил с «Хабра» за вольное допущение :)

Такие алгоритмы называются галактическими, потому что дают выигрыш только на масштабах, нерелевантных для нас. А в программном умножении матриц, если я правильно помню курс алгоритмов и умею читать «Википедию», очень любят алгоритм Штрассена с его _O(2,807)_ и маленькими константами. Но и те, к сожалению, жрут слишком много памяти.



# А теперь когда мозг к херам отсох. Вот ответ на вопрос: А сколько это все стоит:


| Структура данных | Вставка | Удаление | Чтение |
| ---------------- | ------- | -------- | ------ |
| Stack            | 1       | 1        | n      |
| Queue            | 1       | 1        | n      |
| Dequeue          | 1       | 1        | n      |
| List             | n       | n        | n      |
| Set              | 1       | 1        | n      |
| Ordered Set      | log(n)  | log(n)   | log(n) |
| Hash Table       | 1       | 1        | 1      |



