
(GIL, Global Interpreter Lock - глобальная блокировка интерпретатора) — **способ синхронизации потоков, который используется в некоторых интерпретируемых языках программирования, например в Python и Ruby**. 

В случае Python GIL обеспечивает, при использовании интерпретатора CPython, безопасную работу с потоками. Но из-за GIL в конкретный момент времени выполнять байт-код Python может лишь один поток операционной системы. В результате нельзя ускорить Python-код, интенсивно использующий ресурсы процессора, распределив вычислительную нагрузку по нескольким потокам. Негативное влияние GIL на производительность Python-программ, правда, на этом не заканчивается. Так, GIL создаёт дополнительную нагрузку на систему. Это замедляет многопоточные программы и, что выглядит достаточно неожиданно, может даже оказать влияние на потоки, производительность которых ограничена подсистемой ввода/вывода.

### Потоки операционной системы, потоки Python и GIL

Для начала давайте вспомним о том, что такое потоки Python, и о том, как в Python устроена многопоточность. Когда запускают исполняемый файл `python` — ОС создаёт новый процесс с одним вычислительным потоком, который называется главным потоком. Как и в случае с любой другой С-программой, главный поток начинает выполнение программы `python` с входа в её функцию `main()`. Следующие действия главного потока могут быть сведены к трём шагам:

1. [Инициализация интерпретатора](https://tenthousandmeters.com/blog/python-behind-the-scenes-3-stepping-through-the-cpython-source-code/).

2. [Компиляция Python-кода в байт-код](https://tenthousandmeters.com/blog/python-behind-the-scenes-2-how-the-cpython-compiler-works/).

3. [Вход в вычислительный цикл для выполнения байт-кода](https://tenthousandmeters.com/blog/python-behind-the-scenes-4-how-python-bytecode-is-executed/).

Главный поток — это обычный поток операционной системы, который выполняет скомпилированный C-код. Состояние этого потока включает в себя значения регистров процессора и стек вызова C-функций. А Python-поток должен обладать сведениями о стеке вызовов Python-функций, об исключениях, и о других вещах, имеющих отношение к Python. Для того чтобы всё так и было, CPython помещает всё это в [структуру](https://github.com/python/cpython/blob/5d28bb699a305135a220a97ac52e90d9344a3004/Include/cpython/pystate.h#L51), предназначенную для хранения состояния потока, и связывает состояние Python-потока с потоком операционной системы. Другими словами: `Python-поток = Поток ОС + Состояние Python-потока`.

Вычислительный цикл — это бесконечный цикл, который содержит оператор `switch` огромных размеров, умеющий реагировать на все возможные инструкции, встречающиеся в байт-коде. Для входа в этот цикл поток должен удерживать глобальную блокировку интерпретатора. Главный поток захватывает GIL в ходе инициализации, поэтому он может свободно войти в этот цикл. Когда он входит в цикл — он просто начинает, одну за другой, выполнять инструкции байт-кода, задействуя оператор `switch`.

Время от времени потоку нужно приостановить исполнение байт-кода. Поток, в начале каждой итерации вычислительного цикла, проверяет, имеются ли какие-нибудь причины для остановки выполнения байт-кода. Нам интересна одна из таких причин, которая заключается в том, что другой поток хочет захватить GIL.

В однопоточной Python-программе главный поток — это ещё и единственный поток. Он никогда не освобождает глобальную блокировку интерпретатора. А что же происходит в многопоточных программах? Воспользуемся стандартным модулем [threading](https://docs.python.org/3/library/threading.html) для создания нового Python-потока:

```python

import threading

def f(a, b, c):
    # делаем что-нибудь
    pass

t = threading.Thread(target=f, args=(1, 2), kwargs={'c': 3})
t.start()

```

Метод `start()` экземпляра класса `Thread` создаёт новый поток ОС. В Unix-подобных системах, включая Linux и macOS, данный метод вызывает для этой цели функцию [pthread_create()](https://man7.org/linux/man-pages/man3/pthread_create.3.html). Только что созданный поток начинает выполнение функции `t_bootstrap()` с аргументом `boot`. Аргумент [boot](https://github.com/python/cpython/blob/5d28bb699a305135a220a97ac52e90d9344a3004/Modules/_threadmodule.c#L1019) — это структура, которая содержит целевую функцию, переданные ей аргументы и состояние потока для нового потока ОС. Функция [t_bootstrap()](https://github.com/python/cpython/blob/5d28bb699a305135a220a97ac52e90d9344a3004/Modules/_threadmodule.c#L1029) решает множество задач, но, что важнее всего, она захватывает GIL и входит в вычислительный цикл для выполнения байт-кода вышеупомянутой целевой функции.

Поток, прежде чем захватить GIL, сначала проверяет, удерживает ли GIL какой-то другой поток. Если это не так — поток сразу же захватывает GIL. В противном случае он ждёт до тех пор, пока глобальная блокировка интерпретатора не будет освобождена. Ожидание продолжается в течение фиксированного временного интервала, называемого интервалом переключения (по умолчанию — 5 мс). Если GIL за это время не освободится, поток устанавливает флаги `eval_breaker` и `gil_drop_request`. Флаг `eval_breaker` сообщает потоку, удерживающему GIL, о том, что ему нужно приостановить выполнение байт-кода. А флаг `gil_drop_request` объясняет ему причину необходимости это сделать. Поток, удерживающий GIL, видит эти флаги, начиная следующую итерацию вычислительного цикла, после чего освобождает GIL. Он уведомляет об этом потоки, ожидающие освобождения GIL, а потом один из этих потоков захватывает GIL. Решение о том, какой именно поток нужно разбудить, принимает операционная система, поэтому это может быть тот поток, что установил флаги, а может быть и какой-то другой поток.

Собственно говоря, это — абсолютный минимум сведений, которые нам нужно знать о GIL. А теперь я собираюсь рассказать о том, как GIL влияет на производительность Python-программ. Если то, что вы обнаружите в следующем разделе, покажется вам интересным, вас могут заинтересовать и следующие части этой статьи, где мы подробнее рассмотрим некоторые аспекты GIL.

### Последствия существования GIL

Первое последствие существования GIL широко известно: это невозможность параллельного выполнения Python-потоков. А значит — многопоточные программы, даже на многоядерных машинах, работают не быстрее, чем их однопоточные эквиваленты.

Рассмотрим следующую функцию, производительность которой зависит от скорости процессора. Она выполняет операцию декремента переменной заданное количество раз:

```python

def countdown(n):
    while n > 0:
        n -= 1

```

Представим, что нам нужно выполнить 100,000,000 операций декрементирования переменной. Мы можем запустить `countdown(100_000_000)` в одном потоке, или `countdown(50_000_000)` в двух потоках, или `countdown(25_000_000)` в четырёх потоках и так далее. В языках, где нет GIL, вроде C, мы, увеличивая число потоков, смогли бы наблюдать ускорение вычислений. Я запустил Python-код на своём MacBook Pro. В моём распоряжении были два ядра и технология [hyper-threading](http://www.lighterra.com/papers/modernmicroprocessors/). Вот что у меня получилось:

| **Количество потоков** | **Операций декрементирования на поток (n)** | **Время в секундах (лучшее из 3 попыток)** |
| ---------------------- | ------------------------------------------- | ------------------------------------------ |
| 1                      | 100,000,000                                 | 6.52                                       |
| 2                      | 50,000,000                                  | 6.57                                       |
| 4                      | 25,000,000                                  | 6.59                                       |
| 8                      | 12,500,000                                  | 6.58                                       |

Сколько потоков мы не использовали бы, время выполнения вычислений, в сущности, остаётся одним и тем же. На самом деле, многопоточные варианты программы могут оказаться даже медленнее однопоточного из-за дополнительной нагрузки на систему, вызванной операциями [переключения контекста](https://en.wikipedia.org/wiki/Context_switch). Стандартный интервал переключения составляет 5 мс, в результате переключения контекста выполняются не слишком часто. Но если уменьшить этот интервал, мы увидим замедление многопоточных вариантов программы. Ниже мы поговорим о том, зачем может понадобиться уменьшать интервал переключения.

Хотя использование Python-потоков не может помочь нам в деле ускорения программ, интенсивно использующих ресурсы процессора, потоки могут принести пользу в том случае, когда нужно одновременно выполнять множество операций, производительность которых привязана к подсистеме ввода/вывода. Представим себе сервер, который ожидает входящих подключений и, когда к нему подключается клиентская система, запускает функцию-обработчик в отдельном потоке. Эта функция «общается» с клиентом, считывая данные из клиентского сокета и записывая данные в сокет. При чтении данных функция бездействует до тех пор, пока клиент ей что-нибудь не отправит. Именно в подобных ситуациях многопоточность оказывается очень кстати: пока один поток бездействует, другой может сделать что-то полезное.

Для того чтобы позволить другому потоку выполнить код в то время, когда поток, удерживающий GIL, ожидает выполнения операции ввода/вывода, в CPython все операции ввода/вывода реализованы с использованием следующего паттерна:

1. Освобождение GIL.

2. Выполнение операции, например, [write()](https://man7.org/linux/man-pages/man2/write.2.html), [recv()](https://man7.org/linux/man-pages/man2/recv.2.html), [accept()](https://man7.org/linux/man-pages/man2/accept.2.html).

3. Захват GIL.


Получается, что поток может добровольно освободить GIL, ещё до того, как другой поток установит флаги `eval_breaker` и `gil_drop_request`. Обычно потоку нужно удерживать GIL только тогда, когда он работает с Python-объектами. В результате в CPython паттерн «освобождение-выполнение-захват» реализован не только для операций ввода-вывода, но и для других блокирующих вызовов ОС, вроде [select()](https://man7.org/linux/man-pages/man2/select.2.html) и [pthread_mutex_lock()](https://linux.die.net/man/3/pthread_mutex_lock), а так же для кода, выполняющего «тяжёлые» вычисления на чистом C. Например, хэш-функции в стандартном модуле [hashlib](https://docs.python.org/3/library/hashlib.html) освобождают GIL. Это позволяет нам реально ускорить Python-код, который вызывает подобные функции с использованием многопоточности.

Предположим, что нам нужно вычислить хэши SHA-256 для восьми 128-мегабайтных сообщений. Мы можем вызвать `hashlib.sha256(message)` для каждого сообщения, обойдясь одним потоком, но можно и распределить нагрузку по нескольким потокам. Вот результаты исследования этой задачи, полученные на моём компьютере:

| **Количество потоков** | **Общий размер сообщений на поток** | **Время в секундах (лучшее из 3 попыток)** |
| ---------------------- | ----------------------------------- | ------------------------------------------ |
| 1                      | 1 Гб                                | 3.30                                       |
| 2                      | 512 Мб                              | 1.68                                       |
| 4                      | 256 Мб                              | 1.50                                       |
| 8                      | 128 Мб                              | 1.60                                       |

Переход от одного потока к двум даёт ускорение почти в 2 раза из-за того, что эти два потока работают параллельно. Правда, дальнейшее увеличение числа потоков не особенно сильно улучшает ситуацию, так как на моём компьютере всего два физических процессорных ядра. Тут можно сделать вывод о том, что, прибегнув к многопоточности, можно ускорить Python-код, выполняющий «тяжёлые» вычисления, в том случае, если в этом коде осуществляется вызов C-функций, которые освобождают GIL. Обратите внимание на то, что подобные функции можно обнаружить не только в стандартной библиотеке, но и в модулях сторонних разработчиков, рассчитанных на серьёзные вычисления, вроде NumPy. Можно даже самостоятельно писать [C-расширения, освобождающие GIL](https://docs.python.org/3/c-api/init.html?highlight=gil#releasing-the-gil-from-extension-code).

Мы упоминали о потоках, скорость работы которых привязана к производительности CPU, то есть — о потоках, которые, большую часть времени, заняты некими вычислениями. Мы говорили и о потоках, производительность которых ограничена подсистемой ввода/вывода — о тех, которые большую часть времени заняты ожиданием операций ввода/вывода. Самые интересные последствия существования GIL появляются при смешанном использовании и тех и других потоков. Рассмотрим простой эхо-сервер TCP, который ожидает входящих подключений. Когда к нему подключается клиент — он запускает новый поток для работы с этим клиентом:

```python

from threading import Thread
import socket

def run_server(host='127.0.0.1', port=33333):
    sock = socket.socket()
    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    sock.bind((host, port))
    sock.listen()
    while True:
        client_sock, addr = sock.accept()
        print('Connection from', addr)
        Thread(target=handle_client, args=(client_sock,)).start()

def handle_client(sock):
    while True:
        received_data = sock.recv(4096)
        if not received_data:
            break
        sock.sendall(received_data)

    print('Client disconnected:', sock.getpeername())
    sock.close()

if name == 'main':
    run_server()

```

Сколько запросов в секунду «потянет» этот сервер? Я написал простую программу-клиент, которая, настолько быстро, насколько это возможно, отправляет серверу 1-байтовые сообщения и принимает их от него. У меня получилось что-то около 30 тысяч запросов в секунду (RPS, Requests Per Second). Это, скорее всего, не особенно надёжный результат, так как и сервер, и клиент работали на одном и том же компьютере. Но тут к надёжности этого результата я и не стремился. А интересовало меня то, как упадёт RPS в том случае, если сервер будет, во время обработки запросов клиентов, выполнять в отдельном потоке какую-нибудь серьёзную вычислительную задачу.

Рассмотрим тот же самый серверный код, к которому теперь добавлен код, запускающий дополнительный поток, устроенный довольно примитивно. Код, выполняемый в этом потоке, инкрементирует и декрементирует переменную в бесконечном цикле (при выполнении любого кода, интенсивно использующего ресурсы процессора, в сущности, происходит то же самое):

```python

# ... тот же самый код сервера

def compute():
    n = 0
    while True:
        n += 1
        n -= 1

if name == 'main':
    Thread(target=compute).start()
    run_server()

```

Как думаете — насколько сильно изменится RPS? Упадёт лишь немного? Или, может, снизится в 2 раза? А может — в 10? Нет. Показатель RPS упал до 100, что в 300 раз меньше первоначального показателя. И это крайне удивительно для того, кто привык к тому, как операционная система планирует выполнение потоков. Для того чтобы проиллюстрировать то, что я имею в виду, давайте запустим код сервера и код потока, выполняющего вычисления, в виде отдельных процессов, что приведёт к тому, что на них не будет действовать GIL. Можно разделить код на два отдельных файла, или просто воспользоваться стандартным модулем `multiprocessing` для создания новых процессов. Например, это может выглядеть так:

```python

from multiprocessing import Process

#... тот же самый код сервера

if name == 'main':
    Process(target=compute).start()
    run_server()

```

Этот код выдаёт около 20 тысяч RPS. Более того, если запустить два, три или четыре процесса, интенсивно использующих процессор, RPS почти не меняется. Планировщик ОС отдаёт приоритет процессам, производительность которых привязана к подсистеме ввода/вывода. И это правильно.

В нашем примере серверного кода поток, привязанный к подсистеме ввода/вывода, ожидает, когда сокет будет готов к чтению и записи, но производительность любого другого подобного потока будет ухудшаться по тому же сценарию. Представим себе поток, отвечающий за работу пользовательского интерфейса, который ожидает пользовательского ввода. Он, если рядом с ним запустить поток, интенсивно использующий процессор, будет регулярно «подвисать». Ясно, что обычные потоки операционной системы работают не так, и что причиной этого является GIL. Глобальная блокировка интерпретатора мешает планировщику ОС.

Разработчики CPython, на самом деле, хорошо осведомлены об этой проблеме. Они называют её «эффектом сопровождения» (convoy effect). Дэвид Бизли сделал об этом [доклад](https://www.youtube.com/watch?v=Obt-vMVdM8s) в 2010 году и открыл [обращение о проблеме](https://bugs.python.org/issue7946) на bugs.python.org. Через 11 лет, в 2021 году, это обращение было закрыто. Но проблема так и не была исправлена. Далее мы попытаемся разобраться с тем, почему это так.

[там есть ещё](https://habr.com/ru/companies/wunderfund/articles/586360/)